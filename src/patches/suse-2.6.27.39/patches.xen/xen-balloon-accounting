From: jbeulich@novell.com
Subject: improve interaction with mm's accounting
References: bnc#509753
Patch-mainline: obsolete

--- sle11-2009-08-26.orig/arch/x86/kernel/e820-xen.c	2009-08-31 11:47:11.000000000 +0200
+++ sle11-2009-08-26/arch/x86/kernel/e820-xen.c	2009-06-25 14:20:32.000000000 +0200
@@ -1221,6 +1221,11 @@ int __init e820_find_active_region(const
 {
 	u64 align = PAGE_SIZE;
 
+#ifdef CONFIG_XEN
+	if (last_pfn > xen_start_info->nr_pages)
+		last_pfn = xen_start_info->nr_pages;
+#endif
+
 	*ei_startpfn = round_up(ei->addr, align) >> PAGE_SHIFT;
 	*ei_endpfn = round_down(ei->addr + ei->size, align) >> PAGE_SHIFT;
 
@@ -1255,6 +1260,10 @@ void __init e820_register_active_regions
 					    start_pfn, last_pfn,
 					    &ei_startpfn, &ei_endpfn))
 			add_active_range(nid, ei_startpfn, ei_endpfn);
+#ifdef CONFIG_XEN
+	BUG_ON(nid);
+	add_active_range(nid, last_pfn, last_pfn);
+#endif
 }
 
 /*
--- sle11-2009-08-26.orig/arch/x86/mm/init_32-xen.c	2009-08-31 11:47:11.000000000 +0200
+++ sle11-2009-08-26/arch/x86/mm/init_32-xen.c	2009-08-26 12:11:50.000000000 +0200
@@ -399,8 +399,7 @@ static void __init add_one_highpage_init
 {
 	ClearPageReserved(page);
 	init_page_count(page);
-	if (pfn < xen_start_info->nr_pages)
-		__free_page(page);
+	__free_page(page);
 	totalhigh_pages++;
 }
 
@@ -450,8 +449,16 @@ void __init add_highpages_with_active_re
 #ifndef CONFIG_NUMA
 static void __init set_highmem_pages_init(void)
 {
+	unsigned long pfn;
+
 	add_highpages_with_active_regions(0, highstart_pfn, highend_pfn);
 
+	/* XEN: init high-mem pages outside initial allocation. */
+	for (pfn = xen_start_info->nr_pages; pfn < highend_pfn; pfn++) {
+		ClearPageReserved(pfn_to_page(pfn));
+		init_page_count(pfn_to_page(pfn));
+	}
+
 	totalram_pages += totalhigh_pages;
 }
 #endif /* !CONFIG_NUMA */
@@ -1006,11 +1013,10 @@ void __init mem_init(void)
 #endif
 	/* this will put all low memory onto the freelists */
 	totalram_pages += free_all_bootmem();
-	/* XEN: init and count low-mem pages outside initial allocation. */
+	/* XEN: init low-mem pages outside initial allocation. */
 	for (pfn = xen_start_info->nr_pages; pfn < max_low_pfn; pfn++) {
 		ClearPageReserved(pfn_to_page(pfn));
 		init_page_count(pfn_to_page(pfn));
-		totalram_pages++;
 	}
 
 	reservedpages = 0;
--- sle11-2009-08-26.orig/arch/x86/mm/init_64-xen.c	2009-08-31 11:47:11.000000000 +0200
+++ sle11-2009-08-26/arch/x86/mm/init_64-xen.c	2009-08-26 12:11:53.000000000 +0200
@@ -1085,11 +1085,10 @@ void __init mem_init(void)
 #else
 	totalram_pages = free_all_bootmem();
 #endif
-	/* XEN: init and count pages outside initial allocation. */
+	/* XEN: init pages outside initial allocation. */
 	for (pfn = xen_start_info->nr_pages; pfn < max_pfn; pfn++) {
 		ClearPageReserved(pfn_to_page(pfn));
 		init_page_count(pfn_to_page(pfn));
-		totalram_pages++;
 	}
 	reservedpages = max_pfn - totalram_pages -
 					absent_pages_in_range(0, max_pfn);
--- sle11-2009-08-26.orig/drivers/xen/balloon/balloon.c	2009-06-29 15:44:49.000000000 +0200
+++ sle11-2009-08-26/drivers/xen/balloon/balloon.c	2009-08-31 11:47:38.000000000 +0200
@@ -37,6 +37,7 @@
 #include <linux/sched.h>
 #include <linux/errno.h>
 #include <linux/mm.h>
+#include <linux/swap.h>
 #include <linux/mman.h>
 #include <linux/smp_lock.h>
 #include <linux/pagemap.h>
@@ -81,11 +82,7 @@ struct balloon_stats balloon_stats;
 /* We increase/decrease in batches which fit in a page */
 static unsigned long frame_list[PAGE_SIZE / sizeof(unsigned long)];
 
-/* VM /proc information for memory */
-extern unsigned long totalram_pages;
-
 #ifdef CONFIG_HIGHMEM
-extern unsigned long totalhigh_pages;
 #define inc_totalhigh_pages() (totalhigh_pages++)
 #define dec_totalhigh_pages() (totalhigh_pages--)
 #else
@@ -121,29 +118,44 @@ static struct timer_list balloon_timer;
 	printk(KERN_WARNING "xen_mem: " fmt, ##args)
 
 /* balloon_append: add the given page to the balloon. */
-static void balloon_append(struct page *page)
+static void balloon_append(struct page *page, int account)
 {
+	unsigned long pfn;
+
 	/* Lowmem is re-populated first, so highmem pages go at list tail. */
 	if (PageHighMem(page)) {
 		list_add_tail(PAGE_TO_LIST(page), &ballooned_pages);
 		bs.balloon_high++;
-		dec_totalhigh_pages();
+		if (account)
+			dec_totalhigh_pages();
 	} else {
 		list_add(PAGE_TO_LIST(page), &ballooned_pages);
 		bs.balloon_low++;
 	}
+
+	pfn = page_to_pfn(page);
+	if (account) {
+		SetPageReserved(page);
+		set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+		page_zone(page)->present_pages--;
+	} else {
+		BUG_ON(!PageReserved(page));
+		WARN_ON_ONCE(phys_to_machine_mapping_valid(pfn));
+	}
 }
 
 /* balloon_retrieve: rescue a page from the balloon, if it is not empty. */
-static struct page *balloon_retrieve(void)
+static struct page *balloon_retrieve(int *was_empty)
 {
 	struct page *page;
+	struct zone *zone;
 
 	if (list_empty(&ballooned_pages))
 		return NULL;
 
 	page = LIST_TO_PAGE(ballooned_pages.next);
 	UNLIST_PAGE(page);
+	BUG_ON(!PageReserved(page));
 
 	if (PageHighMem(page)) {
 		bs.balloon_high--;
@@ -151,6 +163,9 @@ static struct page *balloon_retrieve(voi
 	}
 	else
 		bs.balloon_low--;
+	zone = page_zone(page);
+	*was_empty |= !populated_zone(zone);
+	zone->present_pages++;
 
 	return page;
 }
@@ -236,6 +251,7 @@ static int increase_reservation(unsigned
 	unsigned long  pfn, i, flags;
 	struct page   *page;
 	long           rc;
+	int            need_zonelists_rebuild = 0;
 	struct xen_memory_reservation reservation = {
 		.address_bits = 0,
 		.extent_order = 0,
@@ -261,7 +277,7 @@ static int increase_reservation(unsigned
 		goto out;
 
 	for (i = 0; i < rc; i++) {
-		page = balloon_retrieve();
+		page = balloon_retrieve(&need_zonelists_rebuild);
 		BUG_ON(page == NULL);
 
 		pfn = page_to_pfn(page);
@@ -294,6 +310,14 @@ static int increase_reservation(unsigned
  out:
 	balloon_unlock(flags);
 
+#ifndef MODULE
+	setup_per_zone_pages_min();
+	if (need_zonelists_rebuild)
+		build_all_zonelists();
+	else
+		vm_total_pages = nr_free_pagecache_pages();
+#endif
+
 	return rc < 0 ? rc : rc != nr_pages;
 }
 
@@ -352,8 +376,7 @@ static int decrease_reservation(unsigned
 	/* No more mappings: invalidate P2M and add to balloon. */
 	for (i = 0; i < nr_pages; i++) {
 		pfn = mfn_to_pfn(frame_list[i]);
-		set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
-		balloon_append(pfn_to_page(pfn));
+		balloon_append(pfn_to_page(pfn), 1);
 	}
 
 	set_xen_guest_handle(reservation.extent_start, frame_list);
@@ -541,8 +564,11 @@ static int __init balloon_init(void)
 	/* Initialise the balloon with excess memory space. */
 	for (pfn = xen_start_info->nr_pages; pfn < max_pfn; pfn++) {
 		page = pfn_to_page(pfn);
-		if (!PageReserved(page))
-			balloon_append(page);
+		if (!PageReserved(page)) {
+			SetPageReserved(page);
+			set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+			balloon_append(page, 0);
+		}
 	}
 #endif
 
@@ -577,7 +603,7 @@ void balloon_update_driver_allowance(lon
 static int dealloc_pte_fn(
 	pte_t *pte, struct page *pmd_page, unsigned long addr, void *data)
 {
-	unsigned long mfn = pte_mfn(*pte);
+	unsigned long pfn, mfn = pte_mfn(*pte);
 	int ret;
 	struct xen_memory_reservation reservation = {
 		.nr_extents   = 1,
@@ -586,7 +612,9 @@ static int dealloc_pte_fn(
 	};
 	set_xen_guest_handle(reservation.extent_start, &mfn);
 	set_pte_at(&init_mm, addr, pte, __pte_ma(0));
-	set_phys_to_machine(__pa(addr) >> PAGE_SHIFT, INVALID_P2M_ENTRY);
+	pfn = __pa(addr) >> PAGE_SHIFT;
+	set_phys_to_machine(pfn, INVALID_P2M_ENTRY);
+	SetPageReserved(pfn_to_page(pfn));
 	ret = HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);
 	BUG_ON(ret != 1);
 	return 0;
@@ -644,6 +672,9 @@ struct page **alloc_empty_pages_and_page
 		}
 
 		totalram_pages = --bs.current_pages;
+		if (PageHighMem(page))
+			dec_totalhigh_pages();
+		page_zone(page)->present_pages--;
 
 		balloon_unlock(flags);
 	}
@@ -658,7 +689,7 @@ struct page **alloc_empty_pages_and_page
  err:
 	balloon_lock(flags);
 	while (--i >= 0)
-		balloon_append(pagevec[i]);
+		balloon_append(pagevec[i], 0);
 	balloon_unlock(flags);
 	kfree(pagevec);
 	pagevec = NULL;
@@ -676,7 +707,7 @@ void free_empty_pages_and_pagevec(struct
 	balloon_lock(flags);
 	for (i = 0; i < nr_pages; i++) {
 		BUG_ON(page_count(pagevec[i]) != 1);
-		balloon_append(pagevec[i]);
+		balloon_append(pagevec[i], 0);
 	}
 	balloon_unlock(flags);
 
@@ -690,7 +721,7 @@ void balloon_release_driver_page(struct 
 	unsigned long flags;
 
 	balloon_lock(flags);
-	balloon_append(page);
+	balloon_append(page, 1);
 	bs.driver_pages--;
 	balloon_unlock(flags);
 
--- sle11-2009-08-26.orig/mm/page_alloc.c	2009-08-31 11:47:11.000000000 +0200
+++ sle11-2009-08-26/mm/page_alloc.c	2009-07-31 15:14:57.000000000 +0200
@@ -4343,6 +4343,23 @@ static void __setup_per_zone_pages_min(v
 		spin_unlock_irqrestore(&zone->lock, flags);
 	}
 
+#ifdef CONFIG_XEN
+	for_each_zone(zone) {
+		unsigned int cpu;
+
+		if (!populated_zone(zone))
+			continue;
+		for_each_online_cpu(cpu) {
+			unsigned long high;
+
+			high = percpu_pagelist_fraction
+			       ? zone->present_pages / percpu_pagelist_fraction
+			       : 5 * zone_batchsize(zone);
+			setup_pagelist_highmark(zone_pcp(zone, cpu), high);
+		}
+	}
+#endif
+
 	/* update totalreserve_pages */
 	calculate_totalreserve_pages();
 }
